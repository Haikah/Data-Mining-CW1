# -*- coding: utf-8 -*-
"""MLP Optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1874Ecq0MuMP2N2hbAlcWLoKqcjPcZqAV
"""

from google.colab import drive
drive.mount('/content/gdrive')

#MLP using test set
import numpy as np
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
from matplotlib import pyplot as plt
from sklearn import metrics
scaler = StandardScaler()
one_hot = OneHotEncoder()
df_input=pd.read_csv('/content/gdrive/My Drive/DMML CW2/x_train_gr_smpl (1).csv')
#df.head()
X1=df_input.values
X_train = scaler.fit_transform(X1)
#print(X_train.shape)
df_output=pd.read_csv('/content/gdrive/My Drive/DMML CW2/y_train_smpl (1).csv')
Y1=df_output.values
y_train = one_hot.fit_transform(Y1).toarray()
test_input=pd.read_csv('/content/gdrive/My Drive/DMML CW2/x_test_gr_smpl.csv')
X2=test_input.values
X_test = scaler.fit_transform(X2)
test_output=pd.read_csv('/content/gdrive/My Drive/DMML CW2/y_test_smpl.csv')
Y2=test_output.values
Y_test = one_hot.fit_transform(Y2).toarray()
#Let's convert one hot enocded label to labels to help us compare predicted and actual values for certain metrics
actual_output = list()
for i in range(len(Y_test)):
    actual_output.append(np.argmax(Y_test[i]))
#print(Y_train.shape)
# Building our Neural Network using Keras
model = Sequential()
model.add(Dense(100, input_dim=2304, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(10, activation='softmax'))
opt = keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
hist = model.fit(X_train, y_train, epochs=100, batch_size=100)
predicted = model.predict(X_test)
#Let's create a list and append our predictions as lables to that list
predicted_output = list()
for i in range(len(predicted)):
    predicted_output.append(np.argmax(predicted[i]))
#Printing confusion matrix using the actual and predicted values
matrix=confusion_matrix(actual_output,predicted_output)
print(matrix)
#Calculating some important metrics
False_Positive = matrix.sum(axis=0) - np.diag(matrix)  
False_Negative = matrix.sum(axis=1) - np.diag(matrix)
True_Positive = np.diag(matrix)
True_Negative = matrix.sum() - (False_Positive + False_Negative + True_Positive)
True_Positive_Rate = True_Positive/(True_Positive+False_Negative)
True_Negative_Rate = True_Negative/(True_Negative+False_Positive) 
False_Positive_Rate = False_Positive/(False_Positive+True_Negative)
False_Negative_Rate = False_Negative/(True_Positive+False_Negative)
print("True Positive Rate",True_Positive_Rate)
print("True Negative Rate",True_Negative_Rate)
print("False positive Rate",False_Positive_Rate)
print("False Negative Rate",False_Negative_Rate)
#Let's print a overall report
print(classification_report(actual_output,predicted_output))

#MLP using 4000 test instances
import numpy as np
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
from matplotlib import pyplot
scaler = StandardScaler()
one_hot = OneHotEncoder()
df_input=pd.read_csv('/content/gdrive/My Drive/DMML CW2/x_train_gr_smpl (1).csv')
#df.head()
X1=df_input.values
X = scaler.fit_transform(X1)
#print(X_train.shape)
df_output=pd.read_csv('/content/gdrive/My Drive/DMML CW2/y_train_smpl (1).csv')
Y1=df_output.values
Y = one_hot.fit_transform(Y1).toarray()
#print(Y_train.shape)
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.41279669762641)
actual_output = list()
for i in range(len(y_test)):
    actual_output.append(np.argmax(y_test[i]))
# Building our Neural Network using Keras
model = Sequential()
model.add(Dense(100, input_dim=2304, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(10, activation='softmax'))
opt = keras.optimizers.Adam(learning_rate=0.1)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
hist = model.fit(X_train, y_train, epochs=100, batch_size=100)
predicted = model.predict(X_test)
#Let's create a list and append our predictions as lables to that list
predicted_output = list()
for i in range(len(predicted)):
    predicted_output.append(np.argmax(predicted[i]))
#Printing confusion matrix using the actual and predicted values
matrix=confusion_matrix(actual_output,predicted_output)
print(matrix)
#Calculating some important metrics
False_Positive = matrix.sum(axis=0) - np.diag(matrix)  
False_Negative = matrix.sum(axis=1) - np.diag(matrix)
True_Positive = np.diag(matrix)
True_Negative = matrix.sum() - (False_Positive + False_Negative + True_Positive)
True_Positive_Rate = True_Positive/(True_Positive+False_Negative)
True_Negative_Rate = True_Negative/(True_Negative+False_Positive) 
False_Positive_Rate = False_Positive/(False_Positive+True_Negative)
False_Negative_Rate = False_Negative/(True_Positive+False_Negative)
print("True Positive Rate",True_Positive_Rate)
print("True Negative Rate",True_Negative_Rate)
print("False positive Rate",False_Positive_Rate)
print("False Negative Rate",False_Negative_Rate)
#Let's print a overall report
print(classification_report(actual_output,predicted_output))

#MLP using 9000 test instances
import numpy as np
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
sc = StandardScaler()
ohe = OneHotEncoder()
df_input=pd.read_csv('/content/gdrive/My Drive/DMML CW2/x_train_gr_smpl (1).csv')
#df.head()
X1=df_input.values
X = sc.fit_transform(X1)
#print(X_train.shape)
df_output=pd.read_csv('/content/gdrive/My Drive/DMML CW2/y_train_smpl (1).csv')
Y1=df_output.values
Y = ohe.fit_transform(Y1).toarray()
#print(Y_train.shape)
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.9287925696594427)
actual_output = list()
for i in range(len(y_test)):
    actual_output.append(np.argmax(y_test[i]))
# Building our Neural Network using Keras
model = Sequential()
model.add(Dense(100, input_dim=2304, activation='tanh'))
#model.add(Dense(150, activation='tanh'))
#model.add(Dense(150, activation='tanh'))
model.add(Dense(150, activation='tanh'))
model.add(Dense(10, activation='softmax'))
#opt = keras.optimizers.Adam(learning_rate=0.1)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
hist = model.fit(X_train, y_train, epochs=100, batch_size=100)
predicted = model.predict(X_test)
#Let's create a list and append our predictions as lables to that list
predicted_output = list()
for i in range(len(predicted)):
    predicted_output.append(np.argmax(predicted[i]))
#Printing confusion matrix using the actual and predicted values
matrix=confusion_matrix(actual_output,predicted_output)
print(matrix)
#Calculating some important metrics
False_Positive = matrix.sum(axis=0) - np.diag(matrix)  
False_Negative = matrix.sum(axis=1) - np.diag(matrix)
True_Positive = np.diag(matrix)
True_Negative = matrix.sum() - (False_Positive + False_Negative + True_Positive)
True_Positive_Rate = True_Positive/(True_Positive+False_Negative)
True_Negative_Rate = True_Negative/(True_Negative+False_Positive) 
False_Positive_Rate = False_Positive/(False_Positive+True_Negative)
False_Negative_Rate = False_Negative/(True_Positive+False_Negative)
print("True Positive Rate",True_Positive_Rate)
print("True Negative Rate",True_Negative_Rate)
print("False positive Rate",False_Positive_Rate)
print("False Negative Rate",False_Negative_Rate)
#Let's print a overall report
print(classification_report(actual_output,predicted_output))

#MLP using cross validation
import numpy as np
import pandas as pd
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
sc = StandardScaler()
ohe = OneHotEncoder()
folds = 10
acc_per_fold = []
loss_per_fold = []
df_input=pd.read_csv('/content/gdrive/My Drive/DMML CW2/x_train_gr_smpl (1).csv')
#df.head()
X1=df_input.values
X = sc.fit_transform(X1)
#print(X_train.shape)
df_output=pd.read_csv('/content/gdrive/My Drive/DMML CW2/y_train_smpl (1).csv')
Y1=df_output.values
Y = ohe.fit_transform(Y1).toarray()
#print(Y_train.shape)
kfold = KFold(n_splits=folds, shuffle=True)
# Neural network
cvscores = []
for train, test in kfold.split(X, Y):
  model = Sequential()
  model.add(Dense(100, input_dim=2304, activation='relu'))
  model.add(Dense(150, activation='relu'))
  model.add(Dense(10, activation='softmax'))
  history= model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  model.fit(X[train], Y[train], epochs=100, batch_size=100, verbose=0)
	# evaluate the model
  scores = model.evaluate(X[test], Y[test], verbose=0)
  print("Accuracy: %.2f%%", scores[1]*100)
  #print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
  cvscores.append(scores[1] * 100)
print("Average accuracy %.2f "% np.mean(cvscores))